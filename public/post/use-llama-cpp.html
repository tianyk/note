<!DOCTYPE html>
<html lang=zh>
<!-- zh-cmn-Hans -->
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="360-site-verification" content="8902ef2b161a9e0f358117b6fe156521" />
    
    <meta name="description" content="llama.cpp大模型有训练和推理两部分，训练会产生一个大模型文件，这些文件通常包含了模型架构以及每个神经元的权重和偏置值。llama.cpp主要用在推理部分，它是一个是一个使用c++开发的大模型推理框架。它可以在普通家用电脑上完成推理，只需要CPU和几个G的内存就能运行。 编译安装 参考 https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp?tab&#x3D;readme-ov-">
<meta property="og:type" content="article">
<meta property="og:title" content="使用llama.cpp推理大模型">
<meta property="og:url" content="https://kekek.cc/post/use-llama-cpp.html">
<meta property="og:site_name" content="Keke的个人网站">
<meta property="og:description" content="llama.cpp大模型有训练和推理两部分，训练会产生一个大模型文件，这些文件通常包含了模型架构以及每个神经元的权重和偏置值。llama.cpp主要用在推理部分，它是一个是一个使用c++开发的大模型推理框架。它可以在普通家用电脑上完成推理，只需要CPU和几个G的内存就能运行。 编译安装 参考 https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp?tab&#x3D;readme-ov-">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-01-02T02:04:14.000Z">
<meta property="article:modified_time" content="2025-01-15T07:48:24.290Z">
<meta property="article:author" content="tyk">
<meta name="twitter:card" content="summary">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>使用llama.cpp推理大模型</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
      <link rel="alternate" href="/atom.xml" title="Keke的个人网站" type="application/atom+xml" />
    
<meta name="generator" content="Hexo 8.1.1"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
         
          <li><a href="/atom.xml">RSS</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/post/openclaw-ai-my-first-blog.html"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/post/go-sync.html"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="mailto:?subject=使用llama.cpp推理大模型&body=Check out this article: https://kekek.cc/post/use-llama-cpp.html"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://kekek.cc/post/use-llama-cpp.html&title=使用llama.cpp推理大模型"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#llama-cpp"><span class="toc-number">1.</span> <span class="toc-text">llama.cpp</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.</span> <span class="toc-text">编译安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.2.</span> <span class="toc-text">模型转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%92%8Cllama-cpp%E7%B1%BB%E4%BC%BC%E7%9A%84%E5%B7%A5%E5%85%B7"><span class="toc-number">1.3.</span> <span class="toc-text">和llama.cpp类似的工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hugging-Face-%E9%95%9C%E5%83%8F"><span class="toc-number">1.4.</span> <span class="toc-text">Hugging Face 镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A"><span class="toc-number">1.5.</span> <span class="toc-text">名词解释</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        使用llama.cpp推理大模型
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">tyk</span>
      </span>
      
    <div class="postdate">
        <time datetime="2024-01-02T02:04:14.000Z" itemprop="datePublished">2024-01-02</time>
    </div>


      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="llama-cpp"><a href="#llama-cpp" class="headerlink" title="llama.cpp"></a>llama.cpp</h2><p>大模型有训练和推理两部分，训练会产生一个大模型文件，这些文件通常包含了模型架构以及每个神经元的权重和偏置值。<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>主要用在推理部分，它是一个是一个使用<code>c++</code>开发的大模型推理框架。它可以在普通家用电脑上完成推理，只需要CPU和几个G的内存就能运行。</p>
<h3 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h3><blockquote>
<p>参考 <a href="https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#build">https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#build</a></p>
</blockquote>
<ol>
<li><p>拉取代码</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ggerganov/llama.cpp.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>编译</p>
<blockquote>
<p>对于非<code>Apple Silicon</code>系列芯片推理时如果有问题可以在编译时禁用<code>GPU</code>。编译时使用<code>LLAMA_NO_METAL=1</code>或者<code>LLAMA_METAL=OFF</code>参数。<a href="https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#metal-build">https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#metal-build</a></p>
</blockquote>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure></li>
</ol>
<p>编译完成会参数<code>mian</code>和<code>quantize</code>文件，前者用来运行大模型推理，后者用来模型向量化处理。</p>
<h3 id="模型转换"><a href="#模型转换" class="headerlink" title="模型转换"></a>模型转换</h3><p>一般大模型文件都托管在<a href="https://huggingface.co/">Hugging Face</a>上面。一般情况下<code>llama.cpp</code>不能直接使用这些大模型进行推理，我们需要先对这些模型进行转换，转为<code>ggml</code>格式。支持转换的大模型列表参考官方<a href="https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#description">README</a>国产大模型百川、千问等都支持转为<code>ggml</code>格式。</p>
<p>转换使用<code>llama.cpp</code>项目内的<code>convert.py</code>或者<code>convert-hf-to-gguf.py</code>处理，详细步骤参考 <a href="https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#prepare-data--run">https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#prepare-data--run</a>。这里对Python版本有一定的要求，模块的依赖在<code>requirements.txt</code>和<code>requirements</code>目录下。</p>
<p>一般情况先使用<code>convert.py</code>转换，如果转换失败在使用<code>convert-hf-to-gguf.py</code>尝试处理。注意，使用<code>convert-hf-to-gguf.py</code>时需要我们安装额外的依赖，依赖列表在<code>requirements/requirements-convert-hf-to-gguf.txt</code>。</p>
<p>转换后的模型我们需要进行向量化，使用<code>./quantize</code>对转换后的模型进行向量化。向量化后的模型就可以进行推理了。</p>
<h3 id="和llama-cpp类似的工具"><a href="#和llama-cpp类似的工具" class="headerlink" title="和llama.cpp类似的工具"></a>和llama.cpp类似的工具</h3><ul>
<li><p><a href="https://github.com/li-plus/chatglm.cpp">chatglm.cpp</a></p>
<p>  一个用于<a href="https://github.com/THUDM/ChatGLM3">ChatGLM</a>大模型推理框架</p>
</li>
<li><p><a href="https://github.com/SJTU-IPADS/PowerInfer">PowerInfer</a></p>
<ul>
<li>一个可以在消费级GPU运行的大模型推理框架</li>
</ul>
</li>
</ul>
<h3 id="Hugging-Face-镜像"><a href="#Hugging-Face-镜像" class="headerlink" title="Hugging Face 镜像"></a>Hugging Face 镜像</h3><ul>
<li><p><a href="https://www.modelscope.cn/models">modelscope</a></p>
<p>  可以直接使用<code>git clone</code>拉取大模型。注意，需要安装<a href="https://git-lfs.com/">LFS</a></p>
</li>
<li><p><a href="https://hf-mirror.com/">hf-mirror</a></p>
<p>  配置镜像后可以使用<code>huggingface</code>官方命令行<code>huggingface-cli</code>工具下载模型文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><ul>
<li><p>B<br>  大模型权重参数单位，1B表示十亿。例如有<code>7B</code>、<code>14B</code>表示参数有70亿和140亿，参数越大推理时耗费的资源越多</p>
</li>
<li><p>FP16、int8、int4</p>
<p>  FP16、int8、int4 等通常指的是模型权重、激活值或梯度的数值表示方式，表示数值精度。</p>
<p>  FP16 指的是 16 位浮点数（即半精度浮点数），每个数值占用 2 字节。</p>
<p>  int8 指的是 8 位整数，每个数值占用 1 字节。</p>
<p>  1B的FP16精度的大模型为<code>1000,000,000</code> * 2Byte ≈ 2GB，1B的int8精度的大模型为<code>1000,000,000</code> * 1Byte ≈ 1GB</p>
</li>
<li><p>Quantization (量化)</p>
<p>  <code>量化</code> 是一种通过降低数值表示精度（如将 FP32 降为 int8）的技术，以减少内存和计算资源的消耗。</p>
<p>  量化后的模型可以在内存占用和计算复杂度方面更高效，尤其是在推理阶段显著提高速度。</p>
</li>
</ul>

  </div>
  <div class="article-license">
    <!-- 
    <a rel="license" style="text-decoration: none;" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
        <img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" />
    </a>
    <br/> 
    -->
    本站采用「<a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">署名 4.0 国际</a>」进行许可。
</div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>加载评论需要在浏览器启用 JavaScript 脚本支持。</noscript>
        </div>
    </div>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
         
          <li><a href="/atom.xml">RSS</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#llama-cpp"><span class="toc-number">1.</span> <span class="toc-text">llama.cpp</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.</span> <span class="toc-text">编译安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.2.</span> <span class="toc-text">模型转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%92%8Cllama-cpp%E7%B1%BB%E4%BC%BC%E7%9A%84%E5%B7%A5%E5%85%B7"><span class="toc-number">1.3.</span> <span class="toc-text">和llama.cpp类似的工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hugging-Face-%E9%95%9C%E5%83%8F"><span class="toc-number">1.4.</span> <span class="toc-text">Hugging Face 镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A"><span class="toc-number">1.5.</span> <span class="toc-text">名词解释</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="mailto:?subject=使用llama.cpp推理大模型&body=Check out this article: https://kekek.cc/post/use-llama-cpp.html"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://kekek.cc/post/use-llama-cpp.html&title=使用llama.cpp推理大模型"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2026 <span class="heart"></span> tyk <a href="https://beian.miit.gov.cn" target="_blank">京ICP备17056993号-1</a>
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/tags/">标签</a></li>
         
          <li><a href="/search/">搜索</a></li>
         
          <li><a href="/atom.xml">RSS</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>
<!-- styles -->
<!-- 
<link rel="stylesheet" href="/lib/font-awesome/css/fontawesome-all.min.css">
 -->
<!-- 
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">
 -->
<link href="//lib.baomitu.com/font-awesome/5.0.8/web-fonts-with-css/css/fontawesome-all.min.css" rel="stylesheet">
<link href="//lib.baomitu.com/justifiedGallery/3.6.5/css/justifiedGallery.min.css" rel="stylesheet">

<!-- jquery -->
<!-- 
<script src="/lib/jquery/jquery.min.js"></script>
 -->
<!-- 
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
 -->
<script src="//lib.baomitu.com/jquery/3.3.1/jquery.min.js"></script>
<script src="//lib.baomitu.com/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>

<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-110681544-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?eb3776b1457fd82ea128e578d4ca32a7";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'keke-site';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


